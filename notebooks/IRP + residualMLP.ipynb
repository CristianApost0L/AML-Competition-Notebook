{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"},{"sourceId":647698,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":488525,"modelId":503951},{"sourceId":647711,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":488525,"modelId":503951},{"sourceId":647719,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":488525,"modelId":503951}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # IRP-ResidualMLP Experimentation Notebook\n \n This notebook imports all logic from the `src/irp_refiner` package.","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/aml-irp/pytorch/default/3/AML-Competition-Notebook\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# Add src directory to Python path\nimport sys\nimport os\nimport gc\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# This assumes the notebook is in the 'notebooks' directory\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n!mkdir -p \"/kaggle/working/checkpoints\"\n# %%\n# Import our modules\nfrom src.irp_refiner import config\nfrom src.irp_refiner.utils import set_seed\nfrom src.irp_refiner.data_processing import load_and_clean_data\nfrom src.irp_refiner.models.irp import IRPTranslator\nfrom src.irp_refiner.models.mlp import ResidualMLP\nfrom src.irp_refiner.training import train_model\nfrom src.irp_refiner.evaluation import evaluate_retrieval\nfrom src.irp_refiner.baseline_utils import load_data, prepare_train_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# --- 1. Configuration & Setup ---\nworker_init_fn = set_seed(config.SEED)\nDEVICE = config.DEVICE\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# --- 2. Load and Clean Data ---\n# We can load the full, clean dataset once\nX_train_np_cleaned, Y_train_np_cleaned = load_and_clean_data(\n    config.TRAIN_DATA_PATH, config.NOISE_THRESHOLD\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# --- 3. Create a single Train/Val split for this experiment ---\nX_train_np, X_val_np, Y_train_np, Y_val_np = train_test_split(\n    X_train_np_cleaned, Y_train_np_cleaned, \n    test_size=config.VAL_SIZE, \n    random_state=config.SEED\n)\n\nprint(f\"Training samples: {len(X_train_np)}\")\nprint(f\"Validation samples: {len(X_val_np)}\")\n\n# Clean up\ndel X_train_np_cleaned, Y_train_np_cleaned\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# --- 4. Fit IRP Translator (on training data only) ---\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\n# Use anchors from the training set\nanchor_indices = np.random.choice(len(X_train_np), config.K_ANCHORS, replace=False)\nX_anchor = X_train_np[anchor_indices]\nY_anchor = Y_train_np[anchor_indices]\n\nscaler_X = StandardScaler().fit(X_anchor)\nscaler_Y = StandardScaler().fit(Y_anchor)\n\nirp_translator = IRPTranslator(\n    scaler_X, scaler_Y, \n    omega=config.IRP_OMEGA, delta=config.IRP_DELTA, \n    ridge=config.IRP_RIDGE, verbose=True\n)\nirp_translator.fit(X_anchor, Y_anchor)\n\n# Save the translator\nirp_path = f\"{config.CHECKPOINT_DIR}irp_translator_notebook.pkl\"\njoblib.dump(irp_translator, irp_path)\nprint(f\"IRP Translator saved to {irp_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# --- 5. Transform Data & Create DataLoaders ---\nprint(\"Transforming data with IRP...\")\nX_train_IRP = torch.from_numpy(irp_translator.translate(X_train_np)).float()\nX_val_IRP = torch.from_numpy(irp_translator.translate(X_val_np)).float()\n\ntrain_dataset = TensorDataset(X_train_IRP, torch.from_numpy(Y_train_np).float())\nval_dataset = TensorDataset(X_val_IRP, torch.from_numpy(Y_val_np).float())\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=config.BATCH_SIZE, \n    shuffle=True, \n    worker_init_fn=worker_init_fn\n)\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=config.BATCH_SIZE, \n    shuffle=False\n)\n\nprint(\"DataLoaders are ready.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# --- 6. Define Model ---\nmodel = ResidualMLP(\n    input_dim=config.D_X, \n    output_dim=config.D_Y, \n    hidden_dim=config.HIDDEN_DIM,\n    num_hidden_layers=config.NUM_HIDDEN_LAYERS, \n    dropout_p=config.DROPOUT_P\n).to(DEVICE)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# --- 7. Run Training ---\nmodel_path = f\"{config.CHECKPOINT_DIR}mlp_notebook.pth\"\n\nbest_loss = train_model(\n    model,\n    train_loader,\n    val_loader,\n    DEVICE,\n    epochs=config.EPOCHS,\n    lr=config.LR,\n    save_path=model_path,\n    patience=config.EARLY_STOP_PATIENCE,\n    min_delta=config.MIN_IMPROVEMENT_DELTA,\n    resume=True # Set to False to force re-training\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# --- 8. Evaluate Model ---\nprint(\"Loading best model for evaluation...\")\ncheckpoint = torch.load(model_path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# We already have X_val_IRP and Y_val_np\ngt_indices_val = np.arange(len(Y_val_np))\n\n# Get predictions\nall_preds = []\nwith torch.no_grad():\n    for (batch_X,) in DataLoader(TensorDataset(X_val_IRP), batch_size=config.BATCH_SIZE*2):\n        pred_batch = model(batch_X.to(DEVICE)).cpu()\n        all_preds.append(pred_batch)\n        \ntranslated_embd_preds = torch.cat(all_preds, dim=0)\n\n# Run evaluation\nresults = evaluate_retrieval(\n    translated_embd_preds,\n    Y_val_np,\n    gt_indices_val,\n    batch_size=config.BATCH_SIZE\n)\n\nprint(\"\\\\n--- EVALUATION ---\")\nfor metric, value in results.items():\n    if 'recall' in metric:\n        print(f\"  {metric}: {value:.2%}\")\n    else:\n        print(f\"  {metric}: {value:.4f}\")\nprint(\"------------------------------------------\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}