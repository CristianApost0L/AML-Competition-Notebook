{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Image Model Stiching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T12:41:54.487126Z",
     "iopub.status.busy": "2025-11-18T12:41:54.486757Z",
     "iopub.status.idle": "2025-11-18T12:41:55.134307Z",
     "shell.execute_reply": "2025-11-18T12:41:55.133323Z",
     "shell.execute_reply.started": "2025-11-18T12:41:54.487097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/CristianApost0l/AML-Competition-Notebook.git\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/working/AML-Competition-Notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T12:41:55.136508Z",
     "iopub.status.busy": "2025-11-18T12:41:55.136208Z",
     "iopub.status.idle": "2025-11-18T12:42:02.664808Z",
     "shell.execute_reply": "2025-11-18T12:42:02.664060Z",
     "shell.execute_reply.started": "2025-11-18T12:41:55.136476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Add src directory to Python path\n",
    "# This assumes the notebook is in the 'notebooks' directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Checkpoints for both submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T12:42:02.665978Z",
     "iopub.status.busy": "2025-11-18T12:42:02.665609Z",
     "iopub.status.idle": "2025-11-18T12:42:54.783851Z",
     "shell.execute_reply": "2025-11-18T12:42:54.782858Z",
     "shell.execute_reply.started": "2025-11-18T12:42:02.665957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "folder_id = \"1N7KO7zFjJ8PvtwABlRW7Ry5QsBlafTy8\"\n",
    "!gdown --folder $folder_id -O ./checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T12:42:54.786505Z",
     "iopub.status.busy": "2025-11-18T12:42:54.786227Z",
     "iopub.status.idle": "2025-11-18T12:44:08.116937Z",
     "shell.execute_reply": "2025-11-18T12:44:08.115834Z",
     "shell.execute_reply.started": "2025-11-18T12:42:54.786476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r /kaggle/working/AML-Competition-Notebook/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Ensemble (Submission 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T12:44:08.118500Z",
     "iopub.status.busy": "2025-11-18T12:44:08.118158Z",
     "iopub.status.idle": "2025-11-18T12:44:08.373467Z",
     "shell.execute_reply": "2025-11-18T12:44:08.372450Z",
     "shell.execute_reply.started": "2025-11-18T12:44:08.118466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src import config\n",
    "from src.utils import set_seed\n",
    "from src.data_processing import load_and_clean_data\n",
    "from src.models.irp import IRPTranslator\n",
    "from src.models.mlp import ResidualMLP\n",
    "from src.ensembling import EnsembleWrapper\n",
    "from src.evaluation import evaluate_retrieval\n",
    "from src.baseline_utils import load_data, generate_submission\n",
    "from src.training import train_irp_refiner\n",
    "\n",
    "print(\"All modules imported successfully.\")\n",
    "\n",
    "worker_init_fn = set_seed(config.SEED)\n",
    "DEVICE = config.DEVICE\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Kaggle data path: {config.TRAIN_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Clean, and Split Data \n",
    "\n",
    "Load the dataset from the path TRAIN_DATA_PATH specified in the config.py, split it and clean it from noisy captions with NOISE_THRESHOLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T12:44:08.375241Z",
     "iopub.status.busy": "2025-11-18T12:44:08.374596Z",
     "iopub.status.idle": "2025-11-18T12:45:11.348731Z",
     "shell.execute_reply": "2025-11-18T12:45:11.347941Z",
     "shell.execute_reply.started": "2025-11-18T12:44:08.375214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_np_cleaned, Y_train_np_cleaned = load_and_clean_data(\n",
    "    config.TRAIN_DATA_PATH, config.NOISE_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T12:45:11.349834Z",
     "iopub.status.busy": "2025-11-18T12:45:11.349589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Initialize KFold\n",
    "kf = KFold(n_splits=config.K_FOLDS, shuffle=True, random_state=config.SEED)\n",
    "\n",
    "# 2. K-Fold Training Loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_np_cleaned)):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"=============== FOLD {fold+1}/{config.K_FOLDS} ===============\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # --- Split data for this fold ---\n",
    "    X_train_fold, X_val_fold = X_train_np_cleaned[train_idx], X_train_np_cleaned[val_idx]\n",
    "    Y_train_fold, Y_val_fold = Y_train_np_cleaned[train_idx], Y_train_np_cleaned[val_idx]\n",
    "\n",
    "    # --- IRP Stage ---\n",
    "    print(f\"--- FOLD {fold+1}: IRP Stage ---\")\n",
    "    anchor_indices = np.random.choice(len(X_train_fold), config.K_ANCHORS, replace=False)\n",
    "    X_anchor = X_train_fold[anchor_indices]\n",
    "    Y_anchor = Y_train_fold[anchor_indices]\n",
    "\n",
    "    scaler_X = StandardScaler().fit(X_anchor)\n",
    "    scaler_Y = StandardScaler().fit(Y_anchor)\n",
    "\n",
    "    irp_translator_fold = IRPTranslator(\n",
    "        scaler_X, scaler_Y, \n",
    "        omega=config.IRP_OMEGA, delta=config.IRP_DELTA, \n",
    "        ridge=config.IRP_RIDGE, verbose=False\n",
    "    )\n",
    "    irp_translator_fold.fit(X_anchor, Y_anchor)\n",
    "    print(f\"   ✓ IRP translator for fold {fold+1} fitted.\")\n",
    "\n",
    "    irp_path = f\"{config.CHECKPOINT_DIR}irp_translator_fold_{fold}.pkl\"\n",
    "    joblib.dump(irp_translator_fold, irp_path)\n",
    "    print(f\"   ✓ IRP translator saved to {irp_path}\")\n",
    "\n",
    "    X_train_IRP_fold = torch.from_numpy(irp_translator_fold.translate(X_train_fold)).float()\n",
    "    X_val_IRP_fold = torch.from_numpy(irp_translator_fold.translate(X_val_fold)).float()\n",
    "    print(f\"   ✓ Train and Val data transformed for fold {fold+1}.\")\n",
    "\n",
    "    # --- DataLoader Stage ---\n",
    "    train_ds_fold = TensorDataset(X_train_IRP_fold, torch.from_numpy(Y_train_fold).float())\n",
    "    val_ds_fold = TensorDataset(X_val_IRP_fold, torch.from_numpy(Y_val_fold).float())\n",
    "\n",
    "    train_loader_fold = DataLoader(train_ds_fold, batch_size=config.BATCH_SIZE, shuffle=True, worker_init_fn=worker_init_fn)\n",
    "    val_loader_fold = DataLoader(val_ds_fold, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # --- Model Training Stage ---\n",
    "    print(f\"--- FOLD {fold+1}: MLP Refiner Training Stage ---\")\n",
    "    model_fold = ResidualMLP(\n",
    "        input_dim=config.D_X, output_dim=config.D_Y, hidden_dim=config.HIDDEN_DIM,\n",
    "        num_hidden_layers=config.NUM_HIDDEN_LAYERS, dropout_p=config.DROPOUT_P\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    model_path_fold = f\"{config.CHECKPOINT_DIR}mlp_fold_{fold}.pth\"\n",
    "\n",
    "    train_irp_refiner(\n",
    "        model_fold, train_loader_fold, val_loader_fold, config.DEVICE,\n",
    "        epochs=config.EPOCHS, lr=config.LR, save_path=model_path_fold,\n",
    "        patience=config.EARLY_STOP_PATIENCE, min_delta=config.MIN_IMPROVEMENT_DELTA,\n",
    "        resume=False \n",
    "    )\n",
    "\n",
    "    # --- Clean up memory ---\n",
    "    del model_fold, train_loader_fold, val_loader_fold, X_train_IRP_fold, X_val_IRP_fold\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-Fold Training Complete. All models saved.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Model Experimentation (Submission 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T13:39:34.784280Z",
     "iopub.status.busy": "2025-11-18T13:39:34.783430Z",
     "iopub.status.idle": "2025-11-18T13:39:34.791433Z",
     "shell.execute_reply": "2025-11-18T13:39:34.790693Z",
     "shell.execute_reply.started": "2025-11-18T13:39:34.784253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src import config\n",
    "from src import baseline_utils\n",
    "from src import evaluation\n",
    "from src import training\n",
    "from src import ensembling\n",
    "from src.data_processing import load_and_prep_data_direct\n",
    "from src.models import mlp_direct\n",
    "from src.utils import set_seed\n",
    "\n",
    "print(\"All modules imported successfully.\")\n",
    "\n",
    "worker_init_fn = set_seed(config.SEED)\n",
    "DEVICE = config.DEVICE\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Kaggle data path: {config.TRAIN_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Clean, and Split Data \n",
    "\n",
    "Load the dataset from the path TRAIN_DATA_PATH specified in the config.py, split it and clean it from noisy captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T13:39:34.793305Z",
     "iopub.status.busy": "2025-11-18T13:39:34.793073Z",
     "iopub.status.idle": "2025-11-18T13:40:42.475953Z",
     "shell.execute_reply": "2025-11-18T13:40:42.475001Z",
     "shell.execute_reply.started": "2025-11-18T13:39:34.793287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading, cleaning, and splitting data...\")\n",
    "\n",
    "(X_train, y_train, X_val, y_val, \n",
    " val_text_embd, val_img_embd_unique, val_label_gt) = load_and_prep_data_direct(\n",
    "    train_path=config.TRAIN_DATA_PATH,\n",
    "    coco_path=config.MY_DATA_PATH,\n",
    "    use_coco=config.USE_COCO_DATASET,\n",
    "    noise_threshold=config.NOISE_THRESHOLD,\n",
    "    val_split_ratio=config.VAL_SIZE,\n",
    "    random_seed=config.SEED\n",
    ")\n",
    "\n",
    "print(\"\\n--- Data Loading Complete ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val (queries) shape: {X_val.shape}\")\n",
    "print(f\"val_img_embd_unique (gallery) shape: {val_img_embd_unique.shape}\")\n",
    "print(f\"val_label_gt (ground truth) shape: {val_label_gt.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T13:40:42.477542Z",
     "iopub.status.busy": "2025-11-18T13:40:42.477097Z",
     "iopub.status.idle": "2025-11-18T13:40:42.602441Z",
     "shell.execute_reply": "2025-11-18T13:40:42.601499Z",
     "shell.execute_reply.started": "2025-11-18T13:40:42.477510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dl_std = DataLoader(\n",
    "    TensorDataset(X_train, y_train), \n",
    "    batch_size=config.MODERN_HPARAMS['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "val_dl_std = DataLoader(\n",
    "    TensorDataset(X_val, y_val), \n",
    "    batch_size=config.MODERN_HPARAMS['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T13:40:42.603690Z",
     "iopub.status.busy": "2025-11-18T13:40:42.603377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 4. Training Modello E ---\")\n",
    "# This function trains 3 different models and returns them in a list\n",
    "models_E = training.create_direct_ensemble(\n",
    "    X_train, y_train, X_val, y_val, DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Submissions Generation ---\")\n",
    "\n",
    "print(\"\\n Loading K-Fold models and IRP translators...\")\n",
    "\n",
    "model_paths = [f\"{config.CHECKPOINT_DIR}mlp_fold_{f}.pth\" for f in range(config.K_FOLDS)]\n",
    "irp_paths = [f\"{config.CHECKPOINT_DIR}irp_translator_fold_{f}.pkl\" for f in range(config.K_FOLDS)]\n",
    "\n",
    "# Check if the files exist first\n",
    "if not os.path.exists(model_paths[0]):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ERROR: Model file not found at {model_paths[0]}\")\n",
    "    print(\"Please run 'python scripts/train.py' to train the K-Fold models before running this notebook.\")\n",
    "    print(\"=\"*80)\n",
    "    ensemble_wrapper = None\n",
    "else:\n",
    "    ensemble_wrapper = EnsembleWrapper(model_paths, irp_paths, DEVICE)\n",
    "\n",
    "if ensemble_wrapper:\n",
    "    print(\"\\n--- Generating Ensemble Submission ---\")\n",
    "    \n",
    "    # 1. Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    test_data = load_data(config.TEST_DATA_PATH)\n",
    "    test_embds_raw_np = test_data['captions/embeddings']\n",
    "    print(f\"Test data loaded: {len(test_embds_raw_np)} samples.\")\n",
    "\n",
    "    # 2. Generate predictions using the ensemble\n",
    "    print(\"Applying ensemble pipeline to test data... (this may take a moment)\")\n",
    "    pred_embds_ensemble = ensemble_wrapper.translate(test_embds_raw_np)\n",
    "    print(\"Ensemble predictions generated.\")\n",
    "\n",
    "    # 3. Save submission file\n",
    "    submission_filename = 'submission_K_Fold_IRP_MLP_Refiner.csv'\n",
    "    generate_submission(test_data['captions/ids'], pred_embds_ensemble, submission_filename)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"✅ Submission file '{submission_filename}' generated.\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(\"Skipping submission generation because ensemble was not loaded.\")\n",
    "\n",
    "\n",
    "## Ensemble model Submission 1\n",
    "\n",
    "wrapper_E = ensembling.DirectEnsembleWrapper(models_E, DEVICE)\n",
    "wrapper_submission = wrapper_E\n",
    "# ---------------------------------\n",
    "\n",
    "print(f\"Using model: Diverse Ensemble (E)\")\n",
    "\n",
    "# 1. Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data = baseline_utils.load_data(config.TEST_DATA_PATH)\n",
    "X_test_np = test_data['captions/embeddings']\n",
    "test_ids = test_data['captions/ids']\n",
    "print(f\"Test data loaded: {len(X_test_np)} samples.\")\n",
    "\n",
    "# 2. Generate predictions\n",
    "print(\"Generating predictions on the test set...\")\n",
    "# The wrapper handles normalization internally\n",
    "y_test_pred = wrapper_submission.translate(X_test_np, batch_size=512)\n",
    "\n",
    "# 3. Save submission file\n",
    "submission_filename = \"submission_Ensemble_model.csv\"\n",
    "baseline_utils.generate_submission(test_ids, y_test_pred, submission_filename)\n",
    "\n",
    "print(f\"\\n✅ Submission salvata: {submission_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14220991,
     "sourceId": 117959,
     "sourceType": "competition"
    },
    {
     "datasetId": 8762519,
     "sourceId": 13768226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
