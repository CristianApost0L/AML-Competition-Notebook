{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text-to-Image Model Stiching","metadata":{}},{"cell_type":"markdown","source":"## Clone repository","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/CristianApost0l/AML-Competition-Notebook.git\nimport sys\nsys.path.append(\"/kaggle/working/AML-Competition-Notebook\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup and Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport torch\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\n\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split, KFold\n\n# Add src directory to Python path\n# This assumes the notebook is in the 'notebooks' directory\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\nCHECKPOINTS_LOAD = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install Checkpoints for both submissions\nRun only if you want to use them!","metadata":{}},{"cell_type":"code","source":"!pip install gdown\nfolder_id = \"1N7KO7zFjJ8PvtwABlRW7Ry5QsBlafTy8\"\n!gdown --folder $folder_id -O ./checkpoints\nCHECKPOINTS_LOAD = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install the requirements","metadata":{}},{"cell_type":"code","source":"!pip install -r /kaggle/working/AML-Competition-Notebook/requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## K-Fold Ensemble (Submission 2)","metadata":{}},{"cell_type":"markdown","source":"### Import modules and setup","metadata":{}},{"cell_type":"code","source":"from src import config\nfrom src.utils import set_seed, load_direct_ensemble\nfrom src.data_processing import load_and_clean_data\nfrom src.models.irp import IRPTranslator\nfrom src.models.mlp import ResidualMLP\nfrom src.ensembling import EnsembleWrapper\nfrom src.evaluation import evaluate_retrieval\nfrom src.baseline_utils import load_data, generate_submission\nfrom src.training import train_irp_refiner\n\nprint(\"All modules imported successfully.\")\n\nworker_init_fn = set_seed(config.SEED)\nDEVICE = config.DEVICE\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Kaggle data path: {config.TRAIN_DATA_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load, Clean, and Split Data \n\nLoad the dataset from the path TRAIN_DATA_PATH specified in the config.py, split it and clean it from noisy captions with NOISE_THRESHOLD.","metadata":{}},{"cell_type":"code","source":"X_train_np_cleaned, Y_train_np_cleaned = load_and_clean_data(\n    config.TRAIN_DATA_PATH, config.NOISE_THRESHOLD\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training the model\n\nSkip if you want to use the checkpoints!","metadata":{}},{"cell_type":"code","source":"# 1. Initialize KFold\nkf = KFold(n_splits=config.K_FOLDS, shuffle=True, random_state=config.SEED)\n\n# 2. K-Fold Training Loop\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train_np_cleaned)):\n    print(\"\\n\" + \"=\"*80)\n    print(f\"=============== FOLD {fold+1}/{config.K_FOLDS} ===============\")\n    print(\"=\"*80)\n\n    # --- Split data for this fold ---\n    X_train_fold, X_val_fold = X_train_np_cleaned[train_idx], X_train_np_cleaned[val_idx]\n    Y_train_fold, Y_val_fold = Y_train_np_cleaned[train_idx], Y_train_np_cleaned[val_idx]\n\n    # --- IRP Stage ---\n    print(f\"--- FOLD {fold+1}: IRP Stage ---\")\n    anchor_indices = np.random.choice(len(X_train_fold), config.K_ANCHORS, replace=False)\n    X_anchor = X_train_fold[anchor_indices]\n    Y_anchor = Y_train_fold[anchor_indices]\n\n    scaler_X = StandardScaler().fit(X_anchor)\n    scaler_Y = StandardScaler().fit(Y_anchor)\n\n    irp_translator_fold = IRPTranslator(\n        scaler_X, scaler_Y, \n        omega=config.IRP_OMEGA, delta=config.IRP_DELTA, \n        ridge=config.IRP_RIDGE, verbose=False\n    )\n    irp_translator_fold.fit(X_anchor, Y_anchor)\n    print(f\"   ✓ IRP translator for fold {fold+1} fitted.\")\n\n    irp_path = f\"{config.CHECKPOINT_DIR}irp_translator_fold_{fold}.pkl\"\n    joblib.dump(irp_translator_fold, irp_path)\n    print(f\"   ✓ IRP translator saved to {irp_path}\")\n\n    X_train_IRP_fold = torch.from_numpy(irp_translator_fold.translate(X_train_fold)).float()\n    X_val_IRP_fold = torch.from_numpy(irp_translator_fold.translate(X_val_fold)).float()\n    print(f\"   ✓ Train and Val data transformed for fold {fold+1}.\")\n\n    # --- DataLoader Stage ---\n    train_ds_fold = TensorDataset(X_train_IRP_fold, torch.from_numpy(Y_train_fold).float())\n    val_ds_fold = TensorDataset(X_val_IRP_fold, torch.from_numpy(Y_val_fold).float())\n\n    train_loader_fold = DataLoader(train_ds_fold, batch_size=config.BATCH_SIZE, shuffle=True, worker_init_fn=worker_init_fn)\n    val_loader_fold = DataLoader(val_ds_fold, batch_size=config.BATCH_SIZE, shuffle=False)\n\n    # --- Model Training Stage ---\n    print(f\"--- FOLD {fold+1}: MLP Refiner Training Stage ---\")\n    model_fold = ResidualMLP(\n        input_dim=config.D_X, output_dim=config.D_Y, hidden_dim=config.HIDDEN_DIM,\n        num_hidden_layers=config.NUM_HIDDEN_LAYERS, dropout_p=config.DROPOUT_P\n    ).to(config.DEVICE)\n\n    model_path_fold = f\"{config.CHECKPOINT_DIR}mlp_fold_{fold}.pth\"\n\n    train_irp_refiner(\n        model_fold, train_loader_fold, val_loader_fold, config.DEVICE,\n        epochs=config.EPOCHS, lr=config.LR, save_path=model_path_fold,\n        patience=config.EARLY_STOP_PATIENCE, min_delta=config.MIN_IMPROVEMENT_DELTA,\n        resume=False \n    )\n\n    # --- Clean up memory ---\n    del model_fold, train_loader_fold, val_loader_fold, X_train_IRP_fold, X_val_IRP_fold\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"K-Fold Training Complete. All models saved.\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Direct Model Experimentation (Submission 1)","metadata":{}},{"cell_type":"markdown","source":"### Import modules","metadata":{}},{"cell_type":"code","source":"from src import config\nfrom src import baseline_utils\nfrom src import evaluation\nfrom src import training\nfrom src import ensembling\nfrom src.data_processing import load_and_prep_data_direct\nfrom src.models import mlp_direct\nfrom src.utils import set_seed\n\nprint(\"All modules imported successfully.\")\n\nworker_init_fn = set_seed(config.SEED)\nDEVICE = config.DEVICE\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Kaggle data path: {config.TRAIN_DATA_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load, Clean, and Split Data \n\nLoad the dataset from the path TRAIN_DATA_PATH specified in the config.py, split it and clean it from noisy captions.","metadata":{}},{"cell_type":"code","source":"print(\"Loading, cleaning, and splitting data...\")\n\n(X_train, y_train, X_val, y_val, \n val_text_embd, val_img_embd_unique, val_label_gt) = load_and_prep_data_direct(\n    train_path=config.TRAIN_DATA_PATH,\n    coco_path=config.MY_DATA_PATH,\n    use_coco=config.USE_COCO_DATASET,\n    noise_threshold=config.NOISE_THRESHOLD,\n    val_split_ratio=config.VAL_SIZE,\n    random_seed=config.SEED\n)\n\nprint(\"\\n--- Data Loading Complete ---\")\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"X_val (queries) shape: {X_val.shape}\")\nprint(f\"val_img_embd_unique (gallery) shape: {val_img_embd_unique.shape}\")\nprint(f\"val_label_gt (ground truth) shape: {val_label_gt.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create DataLoaders","metadata":{}},{"cell_type":"code","source":"train_dl_std = DataLoader(\n    TensorDataset(X_train, y_train), \n    batch_size=config.MODERN_HPARAMS['batch_size'], \n    shuffle=True\n)\nval_dl_std = DataLoader(\n    TensorDataset(X_val, y_val), \n    batch_size=config.MODERN_HPARAMS['batch_size'], \n    shuffle=False\n)\nprint(\"DataLoaders created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Training\nSkip if you want to use the checkpoints!","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- 4. Training Modello E ---\")\n# This function trains 3 different models and returns them in a list\nmodels_E = training.create_direct_ensemble(\n    X_train, y_train, X_val, y_val, DEVICE\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Submissions","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- Submissions Generation ---\")\n\nprint(\"\\n Loading K-Fold models and IRP translators...\")\n\nmodel_paths = [f\"{config.CHECKPOINT_DIR}mlp_fold_{f}.pth\" for f in range(config.K_FOLDS)]\nirp_paths = [f\"{config.CHECKPOINT_DIR}irp_translator_fold_{f}.pkl\" for f in range(config.K_FOLDS)]\n\n# Check if the files exist first\nif not os.path.exists(model_paths[0]):\n    print(\"=\"*80)\n    print(f\"ERROR: Model file not found at {model_paths[0]}\")\n    print(\"Please run 'python scripts/train.py' to train the K-Fold models before running this notebook.\")\n    print(\"=\"*80)\n    ensemble_wrapper = None\nelse:\n    ensemble_wrapper = EnsembleWrapper(model_paths, irp_paths, DEVICE)\n\nif ensemble_wrapper:\n    print(\"\\n--- Generating Ensemble Submission ---\")\n    \n    # 1. Load test data\n    print(\"Loading test data...\")\n    test_data = load_data(config.TEST_DATA_PATH)\n    test_embds_raw_np = test_data['captions/embeddings']\n    print(f\"Test data loaded: {len(test_embds_raw_np)} samples.\")\n\n    # 2. Generate predictions using the ensemble\n    print(\"Applying ensemble pipeline to test data... (this may take a moment)\")\n    pred_embds_ensemble = ensemble_wrapper.translate(test_embds_raw_np)\n    print(\"Ensemble predictions generated.\")\n\n    # 3. Save submission file\n    submission_filename = 'submission_K_Fold_IRP_MLP_Refiner.csv'\n    generate_submission(test_data['captions/ids'], pred_embds_ensemble, submission_filename)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(f\"✅ Submission file '{submission_filename}' generated.\")\n    print(\"=\"*50)\nelse:\n    print(\"Skipping submission generation because ensemble was not loaded.\")\n\n\n## Ensemble model Submission 1\n\nif CHECKPOINTS_LOAD:\n    paths = [\"/kaggle/working/checkpoints/ensemble_m1.pth\", \"/kaggle/working/checkpoints/ensemble_m2.pth\", \"/kaggle/working/checkpoints/ensemble_m3.pth\"]\n    models_E = load_direct_ensemble(DEVICE, paths)\n    wrapper_E = ensembling.DirectEnsembleWrapper(models_E, DEVICE)\n    wrapper_submission = wrapper_E\nelse:\n    wrapper_E = ensembling.DirectEnsembleWrapper(models_E, DEVICE)\n    wrapper_submission = wrapper_E\n# ---------------------------------\n\nprint(f\"Using model: Diverse Ensemble (E)\")\n\n# 1. Load test data\nprint(\"Loading test data...\")\ntest_data = baseline_utils.load_data(config.TEST_DATA_PATH)\nX_test_np = test_data['captions/embeddings']\ntest_ids = test_data['captions/ids']\nprint(f\"Test data loaded: {len(X_test_np)} samples.\")\n\n# 2. Generate predictions\nprint(\"Generating predictions on the test set...\")\n# The wrapper handles normalization internally\ny_test_pred = wrapper_submission.translate(X_test_np, batch_size=512)\n\n# 3. Save submission file\nsubmission_filename = \"submission_Ensemble_model.csv\"\nbaseline_utils.generate_submission(test_ids, y_test_pred, submission_filename)\n\nprint(f\"\\n✅ Submission salvata: {submission_filename}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}